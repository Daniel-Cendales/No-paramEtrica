---
title: Taller Preparatorio Parcial \#1
author: 
   - Hernan Supelano
   - Daniel Felipe Cendales
output: 
        pdf_document:
                extra_dependencies: ["enumerate", "cancel"]
---

# Estimación histograma


1. Realizamos una simulación de tamaño 100 de $X \sim \mathcal{N}(\mu = 17, \sigma^2 = 4)$ 

```{r Primer punto}
## Ajustamos los parámetros iniciales
n <- 100                                        # Tamaño de la muestra
mu <- 20                                        # Media teórica              
sigma <- 2                                      # Desv. estándar teórica
set.seed(n)                                     # Fijamos la semilla

# Tomamos la muestra
datos <- rnorm(n = n, mean = mu, sd = sigma)

## Estimación histograma
histograma <- hist(datos, plot = FALSE)
```

a. Notemos que:

```{r punto1.a}
# Extraemos la estimación histograma de la densidad
densidad1 <- histograma$density

# Extraemos los conteos y los extremos de un intervalo
ni <- histograma$counts
cortes <- histograma$breaks[1:2]                # Los dos primeros puntos
b <- diff(cortes)/2                             # Distancia sobre 2
densidad2 <- ni / (2*n*b)                       # Los conteos
                                                
# Comparamos los vectores
all.equal(densidad1, densidad2)                 # Hacemos la comparación
```
                                        
b. Sea $x$ el punto medio de un intervalo. Vamos a plantar dos casos:
- **Caso 1:** la distancia del punto medio de un intervalo a uno de los extremos es $b$, lo que implica que la longitud de los intervalos es de $2b$. Entonces
\begin{eqnarray*}
    n_j &=& \#\{x_i: x - b \leq x_i \leq x + b\} \\
        &=& \#\{x_i: -b \leq x_i - x \leq b\} \\
        &=& \#\left\{x_i: -1 \leq \frac{x_i - x}{b} \leq 1\right\} \\
        &=& \sum\limits_{i = 1}^{n} I_{[-1, 1]}\left(\frac{x_i - x}{b}\right)
\end{eqnarray*}

Y por ende la estimación histograma toma la forma:

\begin{eqnarray*}
    \widehat{f_H}(x) &=& \frac{n_j}{2bn} \\
                     &=& \frac{1}{2bn}\sum\limits_{i = 1}^{n} I_{[-1, 1]}\left(\frac{x_i - x}{b}\right) \\
                     &=& \frac{1}{nb}\sum\limits_{i = 1}^{n}\frac{1}{2}I_{[-1, 1]}\left(\frac{x_i - x}{b}\right)
\end{eqnarray*}

- **Caso 2:** la longitud del intervalo es $b$. Es decir que la distancia del punto medio a uno de los extremos es $b/2$. Entonces 
\begin{eqnarray*}
    n_j &=& \#\left\{x_i: x - \frac{b}{2} \leq x_i \leq x + \frac{b}{2}\right\} \\
        &=& \#\left\{x_i: -\frac{b}{2}\leq x_i - x \leq \frac{b}{2}\right\} \\
        &=& \#\left\{x_i: -\frac{1}{2}\leq \frac{x_i - x}{b} \leq \frac{1}{2}\right\} \\
        &=& \sum\limits_{i = 1}^{n} I_{\left[-\frac{1}{2}, \frac{1}{2}\right]}\left(\frac{x_i - x}{b}\right)
\end{eqnarray*}

Y por ende la estimación histograma toma la forma:

\begin{eqnarray*}
    \widehat{f_H}(x) &=& \frac{n_j}{bn} \\
                     &=& \frac{1}{bn}\sum\limits_{i = 1}^{n} I_{\left[-\frac{1}{2}, \frac{1}{2}\right]}\left(\frac{x_i - x}{b}\right)
\end{eqnarray*}

c. Gráfico de curvas teóricas y sus estimaciones.

Primero debemos calcular el $b$ óptimo. Para ello, recordemos la fórmula (asumimos normalidad sobre $f$)

$$b = 3.491\cdot\hat{\sigma}\cdot n^{-1/3}$$

donde $\hat{\sigma} = \frac{\text{RIC}}{1.35}$

Pero el $b$ óptimo necesita de $\sigma$ y no de $\hat{\sigma}$. Entonces lo primero que hacemos será realizar varias simulaciones para encontrar el valor óptimo.

Calculemos entonces las cantidades necesarias:

```{r simulaciones1c}
# Inicio de las simulaciones
N <- 500                                        # Cantidad de simulaciones
muestras <- matrix(0, nrow = n, ncol = N)
b0s <- NULL                                     # Almacenamos los b's 
                                                
for(i in 1:N)
{
    muestras[, i] <- rnorm(n, mean = mu, sd = sigma)
    RICs <- diff(quantile(muestras[, i], probs = c(1, 3)/4, names = FALSE))
    sigmas_hat <- min(RICs / 1.35, sd(muestras[, i]))
    b0s[i] <- 3.491 * sigmas_hat * n^(-1/3) 
}

# Cálculo del b óptimo como promedio de todos los b's
( b_Optimo <- mean(b0s) )

# b óptimo obtenido de la muestra
min(diff(quantile(datos, probs = c(1, 3)/4, names = FALSE)) / 1.35, sd(datos))
```
Ya calculado el $b$ óptimo, podemos hacer la gráfica de la densidad teórica y la estimación que obtuvimos

```{r gráfico, warning = FALSE}
# A 4 desviaciones estándar (de la media), en una normal, estará el 99%
lImites <- mu + c(-1, 1) * 5 * sigma

# Extremos de los intervalos
puntos <- seq(from = lImites[1] - 1, to = lImites[2] + 1, by = b_Optimo)

# Histograma con el ancho de banda óptimo
Hist <- hist(datos, breaks = puntos, plot = FALSE)

# Curva teórica
rango <- seq(from = lImites[1], to = lImites[2], by = 0.1)
plot(x = rango, y = dnorm(rango, mu, sigma), frame = FALSE, 
     xlab = "X", ylab = "Densidad", type = "l", las = 1, 
     lwd = 2, ylim = c(0, max(Hist$density)), col = "blue", 
     main = bquote("Estimación histograma, b" == .(round(b_Optimo, 3))))

# Gráfico del histograma
lines(x = puntos, y = c(Hist$density, 0), type = "s")

# Leyenda
legend(x = "topright", lwd = 2:1, col = c("blue", "black"), 
       legend = c("Teórica", "Histograma"), bty = "n")
```

Para agregar la curva de $E\left\{\widehat{f_H}(x)\right\}$ vamos a usar la muestras generadas, calculamos el promedio y las varianzas

```{r Repeticiones}
# Obtenemos los histogramas de cada muestra
histogramas <- apply(muestras, 2, function(k){
                         c(hist(k, breaks = puntos, plot = FALSE)$density, 0)
                     })
E_fh <- apply(histogramas, 1, mean)             # Valor esperado
Sd_fh <- apply(histogramas, 1, sd)              # Desviación estándar

plot(x = rango, y = dnorm(rango, mu, sigma), frame = FALSE, 
     xlab = "X", ylab = "Densidad", type = "l", las = 1, 
     lwd = 2, ylim = c(0, max(Hist$density)), col = "blue", 
     main = bquote("Estimación histograma, b" == .(round(b_Optimo, 3))))

# Gráfico del histograma
lines(x = puntos, y = c(Hist$density, 0), type = "s", lty = 2)

# Esperanza
lines(x = puntos, y = E_fh, col = "brown", lwd = 2, type = "l")

# Leyenda
legend(x = "topright", lwd = c(2, 1, 2), col = c("blue", "black", "brown"), 
       legend = c("Teórica", "Histograma", "Esperanza"), bty = "n",
       lty = c(1, 2, 1))
```

Para intentar ver el comportamiento de la varianza, podemos graficar todos los histogramas obtenidos y después agregamos unas "bandas de confianza"

```{r muchos histogramas}
# Todos los histogramas obtenidos
matplot(x = puntos, y = histogramas, type = "s", las = 1,
        col = "gray", lty = 1, frame = FALSE, lwd = 0.5, 
        xlab = "Observados", ylab = "Densidades",
        main = bquote("Histogramas, b" == .(round(b_Optimo, 3))))

# El histograma de la muestra inicial
lines(x = puntos, y = c(Hist$density, 0), col = "black", 
      lwd = 2, type = "s", lty = 5)

# Valor esperado
lines(x = puntos, y = E_fh, col = "brown", lwd = 2, type = "l")

# Estimación teórica
lines(rango, dnorm(rango, mean = mu, sd = sigma), col = "blue")

# "Bandas de confianza"
matlines(x = puntos, y = cbind(E_fh - 1.96 * Sd_fh, 
                               E_fh + 1.96 * Sd_fh), 
         type = "l", col = "cyan2", lty = 1, lwd = 1)

# Leyenda
legend(x = "topright", bty = "n", col = c("brown", "black", "cyan2", "blue"),
       lwd = rep(2:1, each = 2), lty = c(5, 1, 1, 1),
       legend = c("Esperanza", "Estimación", "Límites", "Teórica"))
```

d. Gráfico de la curva $V\left\{\widehat{f_H}(x)\right\}$

El gráfico anterior sugería que en los extremos la varianza disminuye y en la parte central es donde más alta es esta. Veamos gráficamente que esto es así:

```{r Varianza}
# Gráfico de la varianza
plot(x = puntos, y = Sd_fh, main = "Gráfico de la varianza", frame = FALSE, 
     xlab = "Observados", ylab = "Valores", type = "l", col = "orange")
```

# Estimación Kernel de la densidad
 
Esta largo :'v

2. **Demostración**

\begin{eqnarray*}
    E\left\{\widehat{f_H}(x)\right\} &=& E\left\{\frac{1}{nb}\sum\limits_{i=1}^{n}K_h\left(\frac{x - X_i}{b}\right)\right\} \\
        &=& \frac{1}{nb}E\left\{\sum\limits_{i=1}^{n}K_H\left(\frac{x - X_i}{b}\right)\right\} \\
        &\overset{\text{linealidad}}{=}& \frac{1}{nb}\sum\limits_{i=1}^{n}E\left\{K_H\left(\frac{x-X_i}{b}\right)\right\} \\
        &\overset{\text{iid}}{=}& \frac{1}{b\cancel{n}}\cancel{n}E\left\{K_H\left(\frac{x - X}{b}\right)\right\} \overset{\text{simetría}}{=}
        \frac{1}{b}E\left\{K_H\left(\frac{X - x}{b}\right)\right\}\\
        &=& \frac{1}{b}\int\limits_{-\infty}^{\infty}K_H\left(\frac{s - x}{b}\right)f_X(s)ds \\
        &\overset{t = \frac{s-x}{b}}{=}& \frac{1}{\cancel{b}}\int\limits_{-\infty}^{\infty}K_H(t)f_X(x + tb)\cancel{b}dt \\
        &\overset{\text{E. Taylor}}{=}& \int\limits_{-\infty}^{\infty}K_H(t)\big[f_X(x) + O(b)\big]dt \\
        &=& \big[f_X(x) + O(b)\big]\cancelto{1}{\int\limits_{-\infty}^{\infty}K_H(t)dt} \\
        &=& f_X(x) + O(b)
\end{eqnarray*}

3. **Demostración:**

\begin{eqnarray*}
    V\left\{\widehat{f_H}(x)\right\} &=& V\left\{\frac{1}{nb}\sum\limits_{i=1}^{n}K_H\left(\frac{X_i-x}{b}\right)\right\} \\
        &\overset{\text{reescalado}}{=}& \frac{1}{n^2}V\left\{\sum\limits_{i=1}^{n}K_{H_b}\left(X_i-x\right)\right\} \\
        &\overset{iid}{=}& \frac{1}{n^{\cancel{2}}}\cancel{n}V\left\{K_{H_b}(X-x)\right\} \\
        &=& \frac{1}{n}E\left\{K_{H_b}(X-x)^2\right\} - \frac{1}{n}\left[E\left\{K_{H_b}(X-x)\right\}\right]^2
\end{eqnarray*}

Notemos que, como $E\left\{K_{H_b}(X-x)\right\}$ existe, implica que existe un $c \in \mathbf{R}^{+}$ tal que $\big|E\left\{K_{H_b}(X-x)\right\} \big| \leq c \implies \big|E\left\{K_{H_b}(X-x)\right\} \big|^2 \leq c^2$.

Por ende podemos deducir que 

\begin{eqnarray*}
    \frac{1}{n}\big|E\left\{K_{H_b}(X-x)\right\} \big|^2 &\leq&  \frac{c^2}{n} \\
        &\implies& \frac{1}{n}\big|E\left\{K_{H_b}(X-x)\right\} \big|^2 = O\left(\frac{1}{n}\right)
\end{eqnarray*}

Nos falta analizar $\frac{1}{n}E\left\{K_{H_b}(X-x)^2\right\}$. Notemos que
\begin{eqnarray*}
    \frac{1}{n}E\left\{K_{H_b}(X-x)^2\right\} &=& \frac{1}{n}E\left\{\frac{1}{b^2}K_{H}\left(\frac{X-x}{b}\right)^2\right\} = \frac{1}{nb^2}E\left\{K_{H}\left(\frac{X-x}{b}\right)^2\right\} \\
        &=& \frac{1}{nb^2}\int\limits_{-\infty}^{\infty}K_H\left(\frac{s-x}{b}\right)^2f_X(s)ds \\
        &\overset{t = \frac{s-x}{b}}{=}& \frac{1}{nb^{\cancel{2}}}\int\limits_{-\infty}^{\infty}K_H(t)^2f_X(x + tb)\cancel{b}dt \\
        &=& \frac{1}{nb}\int\limits_{-\infty}^{\infty}K_H(t)^2\left[f_X(x) + O(b)\right]dt \\
        &=& \left[f_X(x) + O(b)\right]\frac{1}{nb}\int\limits_{-\infty}^{\infty}K_H(t)^2dt \\
        &=& \frac{1}{nb}\left[f_X(x) + O(b)\right]R(K) \\
        &=& \frac{f_X(x)R(K)}{nb} + \frac{R(K)}{nb}O(b) = \frac{f_X(x)R(K)}{nb} + \frac{1}{nb}O(b)
\end{eqnarray*}

El término $O(b)$ expresa que la función original está acotada por la función $kb$ para algún $k \in R$. Luego $\frac{O(b)}{b}$ estará acotado por una constante. Lo que nos lleva a deducir que $\frac{O(b)}{nb}$ está acotado por una múltiplo de $\frac{1}{n}$. Es decir, que $\frac{O(b)}{nb} = O\left(\frac{1}{n}\right)$. Resumiendo lo anterior, tenemos que

$$\frac{1}{n}E\left\{K_{H_b}(X-x)^2\right\} = \frac{f_X(x)R(K)}{nb} + O\left(\frac{1}{n}\right)$$

Que a su vez implica que 

\begin{eqnarray*}
    V\left\{\widehat{f_H}(x)\right\} &=& \frac{f_X(x)R(K)}{nb} + O\left(\frac{1}{n}\right) - O\left(\frac{1}{n}\right)
\end{eqnarray*}

La expresión $O\left(\frac{1}{n}\right) - O(\frac{1}{n})$ es una combinación lineal de múltiplos de funciones de la forma $\frac{1}{n}$ y por ende dicha combinación será acotada por una función de la forma $\frac{1}{n}$. Es decir que $O(\frac{1}{n}) - O\left(\frac{1}{n}\right) = O\left(\frac{1}{n}\right)$. Lo que nos permite completar nuestra demostración, i.e.

$$V\left\{\widehat{f_H}(x)\right\} = \frac{f_X(x)R(K)}{nb} + O\left(\frac{1}{n}\right)$$

4. **Demostración:** sea $X\sim\mathcal{N}(\mu, \sigma^2)$. Tenemos que la función de densidad de $X$ viene dada por:

$$f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}$$

La derivada de esta función viene dada por:

\begin{eqnarray*}
    \frac{d}{dx}f_X(x) &=& \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}\left(-\frac{\cancel{2}(x-\mu)}{\cancel{2}\sigma^2}\right) \\ 
        &=& -\frac{(x-\mu)}{\frac{\sqrt{2}}{\sqrt{2}}\sigma\cdot\sigma}f_X(x) = -\frac{1}{\sqrt{2}\sigma}\frac{x-\mu}{\frac{\sigma}{\sqrt{2}}}f_X(x)
\end{eqnarray*}


Y al elevar el cuadrado la expresión anterior se sigue que

\begin{eqnarray*}
    \left[f_X^{'}(x)\right]^2 &=& \frac{1}{2\sigma^2}\left(\frac{x-\mu}{\frac{\sigma}{\sqrt{2}}}\right)^2f_X(x)^2 \\
        &=& \frac{1}{2\sigma^2}\left(\frac{x-\mu}{\frac{\sigma}{\sqrt{2}}}\right)^2 \frac{1}{\sqrt{2\pi\sigma^2}}\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{2(x-\mu)^2}{2\sigma^2}\right\} \\
        &=& \frac{1}{2\sigma^2\sqrt{2\pi\sigma^2}}\left(\frac{x-\mu}{\frac{\sigma}{\sqrt{2}}}\right)^2 \frac{1}{\sqrt{2\pi 2\frac{\sigma^2}{2}}}\exp\left\{-\frac{(x-\mu)^2}{2\frac{\sigma^2}{2}}\right\} \\
        &=& \frac{1}{2\sigma^2\sqrt{2\pi\sigma^2}\sqrt{2}}\left(\frac{x-\mu}{\frac{\sigma}{\sqrt{2}}}\right)^2 f_{X^*}(x) \\
        &=& \frac{1}{4\sigma^2\sqrt{\pi\sigma^2}}\left(\frac{x-\mu}{\frac{\sigma}{\sqrt{2}}}\right)^2 f_{X^*}(x)
\end{eqnarray*}

La función $f_{X^*}(x)$ se puede entender como la función de densidad de una variable aleatoria normal con media $\mu^* = \mu$ y varianza ${\sigma^*}^2 = \frac{\sigma^2}{2}$. Integrando sobre $x$ la expresión anterior se tiene que
\begin{eqnarray*}
    R(f^{'}) &=& \int\limits_{-\infty}^{\infty}\frac{1}{4\sigma^3\sqrt{\pi}}\left(\frac{x-\mu}{\frac{\sigma}{\sqrt{2}}}\right)^2 f_{X^*}(x)dx \\
        &=& \frac{1}{4\sigma^3\sqrt{\pi}}\int\limits_{-\infty}^{\infty}\left(\frac{x-\mu}{\frac{\sigma}{\sqrt{2}}}\right)^2 f_{X^*}(x)dx 
\end{eqnarray*}

La integral anterior vemos que expresa el cálculo de un valor esperado. Para ser más exactos, estamos calculando $E\left[\left(\frac{X^* - \mu^*}{\sigma^*}\right)^2\right]$. Pero esa transformación corresponde al valor esperado de una variable aleatoria *chi* cuadrado, con un grado de libertad. Como el valor esperado de esta corresponde a sus grados de libertad, se tiene que

\begin{eqnarray*}
    R(f^{'}) &=& \frac{1}{4\sigma^3\sqrt{\pi}}\cancelto{1}{E\left[\left(\frac{X^* - \mu^*}{\sigma^*}\right)^2\right]} = \frac{1}{4\sigma^3\sqrt{\pi}}
\end{eqnarray*}


